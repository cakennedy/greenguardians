# -*- coding: utf-8 -*-
"""Model - Road detection model using Linknet - V4 (k-fold).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R_g6sWpzqWNfWFd3wqqJ-xPn6zrzw5rD

# Intro
This notebook builds a baseline model using the U-Net model architecture and an in-house labeled dataset. This notebook does not involve transfer learning, but it uses augmentation.

## Installing packages
"""

# Commented out IPython magic to ensure Python compatibility.
#!pip install -q ipython-autotime
#!pip install -q timm
#!pip install -q git+https://github.com/qubvel/segmentation_models.pytorch

# %load_ext autotime

"""## Importing Libraries"""

import logging
import torch
import torch.nn as nn
import os
import time
from torch.utils.data import Dataset, DataLoader
from os.path import isfile, join
from osgeo import gdal
import numpy as np
import pandas as pd
from timm.scheduler.cosine_lr import CosineLRScheduler
from tqdm.notebook import tqdm
from torch.utils.tensorboard import SummaryWriter
from torch.optim import Adam, AdamW
from torch.cuda.amp import autocast, GradScaler
import matplotlib.pyplot as plt
import albumentations as A
from albumentations.pytorch import ToTensorV2
from albumentations import ImageOnlyTransform
import random
import shutil
from sklearn.metrics import jaccard_score, f1_score
import cv2 as cv
import segmentation_models_pytorch as smp
from torchvision.utils import make_grid
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.optim.lr_scheduler import CosineAnnealingLR
import glob
import csv
#from google.colab import drive

"""## Setup
Here we connect to a Google Drive and set up a TensorFlow events recorder to later use for metrics visualizations using TensorBoard.
"""

# Commented out IPython magic to ensure Python compatibility.
# Removing old logs
#!rm -rf /data/runs

#drive.mount('/content/drive')
#writer = SummaryWriter("/app/metrics/runs")

# %load_ext tensorboard

"""# Loading Dataset
Downloading in-house labeled dataset from a Google Drive to a local disk that will be directly used for model training.
"""
logging.basicConfig(filename='/app/build_Unet_1.log', level=logging.DEBUG, format='%(asctime)s %(levelname)s: %(message)s')

img_size = 256
data_dir = "/app/data/"
#remote_dir = f"/content/drive/MyDrive/GreenGuardians/data/dataset/osm_road_V2_{img_size}/"
#if not os.path.isdir(data_dir):
    # Dowloading the dataset
#    !cp -nr {remote_dir} data
#    !du -h -d 1 data

exclude_files = []
for file_path in glob.glob(data_dir+"*.csv"):
    with open(file_path, 'r') as csvfile:
        csv_reader = csv.reader(csvfile)
        for row in csv_reader:
            exclude_files.extend(list(set(row)))
exclude_files = list(set(exclude_files))
print(f"{len(exclude_files):,} files are excluded!")
logging.info(f"{len(exclude_files):,} files are excluded!")

# Get the list of files in the source directory
data_files = []
image_source_directory = data_dir+"image/"
mask_source_directory = data_dir+"mask/"
for file_name in os.listdir(image_source_directory):
    if isfile(join(image_source_directory, file_name)) and isfile(join(mask_source_directory, file_name)) and '.tif' in file_name and file_name not in exclude_files:
        data_files.append(file_name)

"""# Augmentation

We applied different types of augmentations, including brightness, contrast, flips, blurring, noise, zooming, and grid shuffling. The object is to increase the dataset size, and make the model learn the underlying pattern to generalize better.
"""

import albumentations as A
import numpy as np
from albumentations.pytorch import ToTensorV2
from albumentations import ImageOnlyTransform
import random

p = 0.5
# Using albumentations, check some examples here : https://albumentations.readthedocs.io/en/latest/examples.html
def trntransforms():
    return A.Compose([
        A.RandomBrightnessContrast(brightness_limit=(-.02, .01), contrast_limit=(-.1, .2), p=1.0),
        A.HorizontalFlip(p=p),
        A.VerticalFlip(p=p),
        A.Transpose(p=p),
        A.Rotate(p=p),
        A.OneOf([
          A.Compose([
            A.RandomScale(scale_limit=[0., .5], p=1.0),
            A.RandomCrop(width=img_size, height=img_size, p=1.0),
          ], p=.25),
          A.GaussNoise(var_limit=(0.00001, 0.000001), p=.15),
          A.GaussianBlur(blur_limit=(3, 7), p=.15),
          A.MotionBlur(blur_limit=3, p=.1),
          A.MedianBlur(blur_limit=5, p=.1),
          A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=.25),
        ], p=p),
        A.RandomGridShuffle(grid=(3, 3), p=.2),
    ], additional_targets={'image0': 'image', 'mask0': 'mask'})

def tsttransforms():
    return A.Compose([])

def cutmix_segmentation(data, target, alpha):
    # Get the batch size
    batch_size = data.size(0)

    # Generate the lambda value from the Beta distribution
    lam = torch.distributions.beta.Beta(alpha, alpha).sample() #.item()

    # Generate random indices for the batch
    rand_index = torch.randperm(batch_size).cuda()

    # Generate the bounding box dimensions
    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)

    # Perform the CutMix augmentation
    data[:, :, bbx1:bbx2, bby1:bby2] = data[rand_index, :, bbx1:bbx2, bby1:bby2]

    # Perform the same operation on the target
    target[:, :, bbx1:bbx2, bby1:bby2] = target[rand_index, :, bbx1:bbx2, bby1:bby2]

    return data, target


def rand_bbox(size, lam):
    W = size[2]
    H = size[3]
    cut_rat = torch.sqrt(1. - lam)
    cut_w = (W * cut_rat).type(torch.long)
    cut_h = (H * cut_rat).type(torch.long)

    # Uniform
    cx = torch.randint(W, (1,))
    cy = torch.randint(H, (1,))

    bbx1 = torch.clamp(cx - cut_w // 2, 0, W)
    bby1 = torch.clamp(cy - cut_h // 2, 0, H)
    bbx2 = torch.clamp(cx + cut_w // 2, 0, W)
    bby2 = torch.clamp(cy + cut_h // 2, 0, H)

    return bbx1, bby1, bbx2, bby2

"""# Dataset
Defining a custom dataset that loads and generates the data after applying augmentations and transformations.
"""

# loadTiff used to read the raster data and you clip and normalize it for display purpose
def loadTiff(in_image, max_val=-1, clip_max=1.0):
    src = gdal.Open(in_image)
    in_band = src.GetRasterBand(1)
    block_xsize, block_ysize = (in_band.XSize, in_band.YSize)
    # read the (multiband) file into an array
    image = src.ReadAsArray(0, 0, block_xsize, block_ysize)
    # Trigger this section for display purpose only
    if max_val != -1:
        nan_count = np.count_nonzero(np.isnan(image))
        if nan_count > 0:
            image = np.nan_to_num(image, nan=0.0)
        for i in range(src.RasterCount):
            image[i] = np.clip(image[i]/max_val, 0, clip_max)
        # reshape from bandsxheightxwidth to wxhxb
        image = np.moveaxis(image, 0, -1)
    return image, block_ysize, block_xsize, src.RasterCount

class RoadDataset(Dataset):
    """Scars Dataset."""

    def __init__(self, data_dir, data_files = [], mean = None, sd=None, transform=None, ratio=1.0, clip_max=1.):
        """Initializing data generator class object"""

        self.transform = transform
        self.clip_max = clip_max
        self.mean = mean
        self.SD = sd

        if len(data_files) == 0:
            print("Pass file names to the dataset")
            logging.info("Pass file names to the dataset")
            return

        # Taking files based on the ratio. Useful for rapid prototype work
        data_files.sort()
        total_files = len(data_files)
        if ratio < 1.0:
            files_to_take = round(len(data_files)*ratio)
            # Making sure there is a minumum of one files
            if files_to_take <= 0:
                files_to_take = 1
            data_files = data_files[:files_to_take]

        image_dir = data_dir+"/image/"
        mask_dir = data_dir+"/mask/"
        bands_data = []
        mask_sum_acc, size_sum = 0.0, 0
        for f in data_files:
            img_f = join(image_dir, f)
            mask_f = join(mask_dir, f)
            [mask, m_xsize, m_ysize, m_nbands] = loadTiff(mask_f)
            mask_sum = mask.sum()
            # Ignore the data if it does not has assocaiated labels
            if mask_sum <= 0:
                continue
            [img, xsize, ysize, nbands] = loadTiff(img_f)
            mask = mask.astype('float32')
            img = img.astype(np.float32)
            img = np.transpose(img, axes=[1, 2, 0])
            # There is a once case where the image has some nan values. We are replacing it with 0 values
            nan_count = np.count_nonzero(np.isnan(img))
            if nan_count > 0:
                #print(f'{image_dir} file has {nan_count} NaN values.')
                img = np.nan_to_num(img, nan=0.0)
            # Clip to the given max size
            img = np.clip(img, 0, self.clip_max)
            # Normalization
            img = img/np.array([self.clip_max]*4)
            img = np.transpose(img, axes=[2, 0, 1]).astype(np.float32)

            width = mask.shape[0]
            height = mask.shape[1]
            mask = mask.flatten()
            mask_sum_acc += mask_sum
            size_sum += np.size(mask)

            band_dict = {
                'image_path': img_f,
                'mask_path': mask_f,
                'red': img[0].flatten(),
                'green': img[1].flatten(),
                'blue': img[2].flatten(),
                'nir': img[3].flatten(),
                'mask': mask, # y
                'mask_sum': mask_sum,
                'width': width,
                'height': height,
            }
            bands_data.append(band_dict)

        self.data = pd.DataFrame(bands_data)
        self.pos_weight = ((size_sum-mask_sum_acc) / size_sum)

        # Mean and SD for standardization
        if self.mean == None or self.SD == None:
            mean, std_dev = [], []
            mean.append(np.mean(np.concatenate(self.data['red'])).astype('float32'))
            mean.append(np.mean(np.concatenate(self.data['green'])).astype('float32'))
            mean.append(np.mean(np.concatenate(self.data['blue'])).astype('float32'))
            mean.append(np.mean(np.concatenate(self.data['nir'])).astype('float32'))
            std_dev.append(np.std(np.concatenate(self.data['red'])).astype('float32'))
            std_dev.append(np.std(np.concatenate(self.data['green'])).astype('float32'))
            std_dev.append(np.std(np.concatenate(self.data['blue'])).astype('float32'))
            std_dev.append(np.std(np.concatenate(self.data['nir'])).astype('float32'))
            self.mean = mean
            self.SD = std_dev
            print(f"Mean {mean}")
            print(f"SD {std_dev}")
            logging.info(f"Mean {mean}")
            logging.info(f"SD {std_dev}")

        print(f'Taking {self.data.shape[0]:,}/{total_files:,} data files. Weight: {self.pos_weight}')
        logging.info(f'Taking {self.data.shape[0]:,}/{total_files:,} data files. Weight: {self.pos_weight}')

    def __len__(self):
        return self.data.shape[0]

    def __getitem__(self, idx):
        """Returns back a data"""

        row = self.data.iloc[idx]
        vals = []
        for ch in ['red', 'green', 'blue', 'nir']:
            v = row[ch].reshape(row['width'], row['height'])
            vals.append(v)
        original_x = vals
        original_x = np.clip(original_x, 0.0, 0.2)
        original_x = (original_x - np.min(original_x)) / (np.max(original_x) - np.min(original_x))
        x = np.stack(vals, axis=-1) # -1 to move the channel at the end

        y = row['mask']
        y = y.reshape(row['width'], row['height'])
        y = np.dstack((y, y, y))
        if self.transform is not None: # If there is no transformation
            transformed = self.transform(image=x, mask=y)
            y = torch.from_numpy(transformed["mask"][:,:,0]).unsqueeze(0)
            x = transformed["image"]
            x = (x - self.mean) / self.SD
            x = torch.from_numpy(np.transpose(x.astype(np.float32), axes=[2, 0, 1]))
        else:
            x = (x - self.mean) / self.SD
            x = torch.from_numpy(np.transpose(x.astype(np.float32), axes=[2, 0, 1]))
            y = torch.from_numpy(y[:,:,0]).unsqueeze(0)

        return x, y, original_x, row["image_path"], row["mask_path"]

"""# Model
Defining a U-Net model whic is a convolutional neural network.
"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#model = smp.Linknet(encoder_name='timm-efficientnet-b4', encoder_depth=5, encoder_weights='imagenet', decoder_use_batchnorm=True,
#                    in_channels=4, classes=1, activation=None, aux_params=None).to(device)
#model = smp.DeepLabV3Plus(encoder_name='resnet101', encoder_depth=5, encoder_weights='imagenet', encoder_output_stride=16, decoder_channels=256, 
#        decoder_atrous_rates=(12, 24, 36), in_channels=4, classes=1, activation=None, upsampling=4, aux_params=None).to(device)
#model = smp.UnetPlusPlus(encoder_name='timm-efficientnet-b4', encoder_depth=5, encoder_weights='imagenet', decoder_use_batchnorm=True, 
#        decoder_channels=(256, 128, 64, 32, 16), decoder_attention_type='scse', in_channels=4, classes=1, activation=None, aux_params=None).to(device)
#model = smp.PSPNet(encoder_name='timm-efficientnet-b4', encoder_weights='imagenet', encoder_depth=3, psp_out_channels=512, psp_use_batchnorm=True, psp_dropout=0.2,
#        in_channels=4, classes=1, activation=None, upsampling=8, aux_params=None).to(device)
model = smp.Unet(encoder_name='timm-efficientnet-b5', encoder_depth=5, encoder_weights='imagenet', decoder_use_batchnorm=True, decoder_channels=(256, 128, 64, 32, 16), 
        decoder_attention_type='scse', in_channels=4, classes=1, activation=None, aux_params=None).to(device)

"""# Loss
Defining a loss which is a combination of weighted binary cross entropy and Dice loss.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class CombinedLossV3(nn.Module):
    def __init__(self, weight=1.0, alpha=0.25, gamma=2., max_loss=10):
        super(CombinedLossV3, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.max_loss = max_loss
        self.beta = weight

    def forward(self, inputs, targets, smooth=1):
        # Flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)

        # Calculate weights for the BCE loss based on the targets
        beta = self.beta
        weights = beta * targets + (1 - beta) * (1 - targets)  # weights add up to 1

        # Weighted BCE Loss
        BCE = F.binary_cross_entropy_with_logits(inputs, targets, weight=weights, reduction='mean')

        # Focal Loss
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss) # pt is the probability of the true class
        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss
        F_loss = F_loss.clamp(max=self.max_loss)  # maintaining numerical stability
        F_loss = torch.mean(F_loss)

        # Dice Loss
        inputs_soft = torch.sigmoid(inputs)
        intersection = (inputs_soft * targets).sum()
        dice = (2.*intersection + smooth)/(inputs_soft.sum() + targets.sum() + smooth)
        dice_loss = 1 - dice

        # Combined Loss
        combined_loss = BCE + F_loss + dice_loss
        return combined_loss

"""# Training Setup
Training preparation by instantiating the model, defining a loss, an optimizer, a scheduler and etc.
"""

# Define the loss function and optimizer
#criterion = CombinedLossV3(train_data.pos_weight)
#optimizer = AdamW(model.parameters(), lr=0.001, weight_decay=0.1)

"""# Training Loop
Defining and running the training loop
"""

def dice_coeff(pred, target, smooth=1):
    pred = pred.contiguous()
    target = target.contiguous()

    intersection = (pred * target).sum(dim=2).sum(dim=2)

    dice = (2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)

    return dice.mean()

def iou(pred, target, smooth=1):
    pred = pred.contiguous()
    target = target.contiguous()

    intersection = (pred * target).sum(dim=2).sum(dim=2)
    total = (pred + target).sum(dim=2).sum(dim=2)

    iou = (intersection + smooth) / (total - intersection + smooth)

    return iou.mean()

class EarlyStopping:
    """Early stops the training if validation loss doesn't improve after a given patience."""
    def __init__(self, patience=7, val_loss_min=float('inf'), verbose=False, delta=0):
        """
        Args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 7
            verbose (bool): If True, prints a message for each validation loss improvement.
                            Default: False
            delta (float): Minimum change in the monitored quantity to qualify as an improvement.
                            Default: 0
        """
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = val_loss_min
        self.delta = delta
        self.best_pt = ''

    def __call__(self, val_loss, model, pt_path):

        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model, pt_path)
        elif score < self.best_score + self.delta:
            self.counter += 1
            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            logging.info(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model, pt_path)
            self.counter = 0

    def save_checkpoint(self, val_loss, model, pt_path):
        '''Saves model when validation loss decreases.'''
        if self.verbose:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')
            logging.info(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')
        torch.save(model, pt_path)  # save checkpoint
        self.val_loss_min = val_loss
        self.best_pt = pt_path

import math

def cosine_annealing(epoch, total_epochs, lr_min, lr_max):
    """
    Implements the Cosine Annealing Learning Rate Schedule.

    Parameters:
        epoch (int): Current epoch number.
        total_epochs (int): Total number of epochs of the training.
        lr_min (float): The minimum learning rate (lower bound of the learning rate).
        lr_max (float): The maximum learning rate (upper bound of the learning rate).

    Returns:
        lr (float): Learning rate for the current epoch.
    """

    lr = lr_min + (lr_max - lr_min) * (1 + math.cos(math.pi * epoch / total_epochs)) / 2
    return lr

import torch
from torch.utils.data import DataLoader, random_split
from sklearn.model_selection import KFold

k = 5
best_pt = ''
def train_model(model_path, model, num_epochs):
    #!rm -rf {model_path}
    #!mkdir -p {model_path}
    step_counter_train, step_counter_val = 0, 0
    #iteration = 0
    mini_epochs = int(num_epochs/k)
    val_loss_min = float('inf')
    lr = 0.001

    # Define k-fold cross validator
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)

    train_metrics = {'epoch': [], 'iou': [], 'dice': [], 'loss': []}
    valid_metrics = {'epoch': [], 'iou': [], 'dice': [], 'loss': []}
    epoch_ocunt = 0
    # K-fold Cross Validation model evaluation
    for fold, (train_ids, test_ids) in enumerate(kfold.split(data_files)):
        print(f'FOLD {fold+1}')
        print('--------------------------------')
        logging.info(f'FOLD {fold+1}')
        logging.info('--------------------------------')

        valid_files = [data_files[i] for i in test_ids]
        train_files = [data_files[i] for i in train_ids]
        train_dataset = RoadDataset(data_dir, train_files, None, None, trntransforms(), 1.)
        valid_dataset = RoadDataset(data_dir, valid_files, train_dataset.mean, train_dataset.SD, tsttransforms(), 1.)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)
        valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, pin_memory=True)

        criterion = CombinedLossV3(train_dataset.pos_weight)
        #early_stopping = EarlyStopping(patience=20, val_loss_min=val_loss_min, verbose=True)
        early_stopping = EarlyStopping(patience=10, val_loss_min=float('inf'), verbose=True)
        # Defining a ReduceLROnPlateau learning rate scheduler
        #lr = cosine_annealing(fold, k, 0.0005, 0.001)
        lr = 0.01
        optimizer = AdamW(model.parameters(), lr=lr)
        #scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=7, verbose=True)
        scheduler = CosineAnnealingLR(optimizer, T_max=int(mini_epochs/2), verbose=True)

        # Run the training loop for defined number of epochs
        for epoch in range(0, mini_epochs):
            print('Epoch {}/{}'.format(epoch+1, mini_epochs))
            logging.info('Epoch {}/{}'.format(epoch+1, mini_epochs))
            epoch_ocunt += 1
            train_metrics['epoch'].append(epoch_ocunt)
            valid_metrics['epoch'].append(epoch_ocunt)
            model.train()
            total_loss, total_dice, total_iou = 0.0, 0.0, 0.0
            for step, batch in enumerate(train_loader):
                x = batch[0].to(device)
                y = batch[1].to(device)

                x, y = cutmix_segmentation(x, y, 0.1)

                optimizer.zero_grad()
                outputs = model(x)
                # Partial loss against original labels, partial loss against mixed up labels
                loss = criterion(outputs, y)

                loss.backward()
                optimizer.step()
                #scheduler.step()

                # Calculate metrics.
                outputs_prob = torch.sigmoid(outputs)
                train_iou = iou(outputs_prob, y).item()
                train_dice = dice_coeff(outputs_prob, y).item()
                train_loss = loss.item()
                total_loss += train_loss
                total_dice += train_dice
                total_iou += train_iou

                step_counter_train += 1
            avg_loss = total_loss / len(train_loader)
            avg_dice = total_dice / len(train_loader)
            avg_iou = total_iou / len(train_loader)
            train_metrics['loss'].append(avg_loss)
            train_metrics['iou'].append(avg_iou)
            train_metrics['dice'].append(avg_dice)
            print(f"Train: Avg Loss = {avg_loss:.5f}, Avg IoU = {avg_iou:.5f}, Avg Dice = {avg_dice:.5F}")
            logging.info(f"Train: Avg Loss = {avg_loss:.5f}, Avg IoU = {avg_iou:.5f}, Avg Dice = {avg_dice:.5F}")

            model.eval()
            total_loss, total_dice, total_iou = 0.0, 0.0, 0.0
            for step, batch in enumerate(valid_loader):
                x = batch[0].to(device)
                y = batch[1].to(device)
                with torch.no_grad():
                    outputs = model(x)
                    loss = criterion(outputs, y)

                # Calculate metrics.
                outputs_prob = torch.sigmoid(outputs)
                val_iou = iou(outputs_prob, y).item()
                val_dice = dice_coeff(outputs_prob, y).item()
                val_loss = loss.item()
                total_loss += val_loss
                total_dice += val_dice
                total_iou += val_iou

                step_counter_val += 1

                # Log the images
                if step_counter_val == 1 or step_counter_val % 100 == 0:  # Only log every 100 batches
                    original_grid = make_grid(batch[2][0][:3,:,:])
                    input_grid = batch[0][0]
                    for i in range(input_grid.shape[0]):
                        input_grid[i] = (input_grid[i] - input_grid[i].min()) / (input_grid[i].max() - input_grid[i].min())
                    input_grid = make_grid(input_grid[:3, :, :])
                    output_grid = make_grid((outputs.detach()[0] > .5).float())
                    label_grid = make_grid(batch[1][0])
            avg_loss = total_loss / len(valid_loader)
            avg_dice = total_dice / len(valid_loader)
            avg_iou = total_iou / len(valid_loader)
            valid_metrics['loss'].append(avg_loss)
            valid_metrics['iou'].append(avg_iou)
            valid_metrics['dice'].append(avg_dice)
            # Step the scheduler
            #scheduler.step(avg_loss)
            scheduler.step()

            print(f"Valid: Avg Loss = {avg_loss:.5f}, Avg IoU = {avg_iou:.5f}, Avg Dice = {avg_dice:.5F}")
            logging.info(f"Valid: Avg Loss = {avg_loss:.5f}, Avg IoU = {avg_iou:.5f}, Avg Dice = {avg_dice:.5F}")
            print(f"Current learning rate: %f" % scheduler.get_last_lr()[0])
            logging.info("Current learning rate: %f" % scheduler.get_last_lr()[0])

            # Early_stopping needs the validation loss to check if it has decreased,
            # and if it has, it will save the current model checkpoint
            pt_path = model_path+f"model_fold_{fold}_epoch_{epoch}_dice_{avg_dice:.5f}_iou_{avg_iou:.5f}_loss_{avg_loss:.5f}.pt"
            early_stopping(avg_loss, model, pt_path)
            best_pt = early_stopping.best_pt

            if early_stopping.early_stop:
                print("Early stopping")
                logging.info("Early stopping")
                val_loss_min = early_stopping.val_loss_min
                break

    # Process is complete.
    print('Training process has finished.')
    logging.info('Training process has finished.')
    print('--------------------------------')
    logging.info('--------------------------------')
    return best_pt, train_metrics, valid_metrics

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=/data/runs

model_path = "/app/model/linknet_k_fold_v4_1/"
model_path = "/app/model/DeepLabV3Plus/"
model_path = "/app/model/PSPNet/"
model_path = "/app/model/Unet/"
best_pt, train_metrics, valid_metrics = train_model(model_path, model, 250)


df = pd.DataFrame(train_metrics)
df.to_csv(model_path+"train_metrics.csv", index=False)
df = pd.DataFrame(valid_metrics)
df.to_csv(model_path+"valid_metrics.csv", index=False)

